{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS559 - F20 Project #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Desciption\n",
    "You are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\n",
    "\n",
    "The task is to predict the value of `target` column in the test set using either **Logistic Regression** and **SVM**. You are welcome to use **regularizaiton**. \n",
    "\n",
    "## File descriptions\n",
    "- train.csv - the training set (202 columns)\n",
    "\n",
    "- test.csv - the test set. The test set contains some rows which are not included in scoring.\n",
    "\n",
    "## Rules\n",
    "- The data does not have specific column names. Therefore, you will not know what data is about. \n",
    "- However, you still can do classicaition problem without clustering the training set. **No unsupervised learning techniques in this project**. \n",
    "- There are 202 columns. This means that the key of high accuracy comes from **EDA** and **feature enegineering**. \n",
    "- There are no rules on EDA and Feature Engineering. \n",
    "- On your model, make sure you can reduce the columns at the most of 25%. If we use all columns, we may have high computational cost and getting into bias-variance tradeoff and underfit vs. overfit situations. \n",
    "- The project is out of 100. \n",
    "    - 50 points will come from your EDA and any pre-processing work. \n",
    "    - 30 points will come from your model: Accuracy + overcoming any ML challenges. \n",
    "    - 10 points will come from in-class competition. \n",
    "        - Ranking the accuracy with less features. \n",
    "    - 10 points will come from a report describing your work flow and model evaluations.\n",
    "        - must be submitted in different file (e.g., pdf, docx). \n",
    "        \n",
    "## Recommand Before-Preprocessing\n",
    "- You can split the set from the data distribution. \n",
    "- You can make multiple new data frames by randomly selecting columns. \n",
    "- You can do similar by rows. \n",
    "\n",
    "## Recommand Before-training model\n",
    "- Make sure to delete features from supportive reasons. \n",
    "\n",
    "Proejct DUE: 10/23/2020 Friday 11:59 PM. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaneesh Kolluri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I pledge my honor that I have abided by the Stevens Honor System.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I will be making my comments to describe my work flow and model evaluations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the import statements for most of the functions used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn as sk\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = df.drop(columns = ['target','ID_code'])\n",
    "y_vals = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was the data for one of the models\n",
    "kbest_and_corr_features = df.drop(columns = ['target','ID_code'])\n",
    "kbest_and_corr_features.loc[:,'var_0':'var_199'] = normalize(df.drop(columns = ['target','ID_code']))\n",
    "y_vals = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4Vals = df.drop(columns = ['target','ID_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5Vals = df.drop(columns = ['target','ID_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6Vals = df.drop(columns = ['target','ID_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I wanted to use the Select From Model in order to detect which features contribute to predict the target value. I wanted to use regression, and Lasso specifically to select variables and regularize the data. Because this would find vectors that minimize the prediction error, by trying to shrink the coefficients to exactly zero. I split my data into two portions and tested to see if we can remove any features in order to save time, but this test was inconclusive. L1 is Lasso.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = normalize(x_vals)\n",
    "x_vals.loc[:,'var_0':'var_199'] = x\n",
    "x1 = x_vals.iloc[:,0:(len(x_vals.columns)//2)]\n",
    "x2 = x_vals.iloc[:,(len(x_vals.columns)//2 +1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the data points from 0-1, giving them an equal range for all.\n",
    "las_scaler = StandardScaler()\n",
    "las_scaler.fit(x1)\n",
    "lasso_reg = SelectFromModel(LogisticRegression(penalty = 'l1'))\n",
    "lasso_reg.fit(las_scaler.transform(x1),y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the data points from 0-1, giving them an equal range for all.\n",
    "las_scaler = StandardScaler()\n",
    "las_scaler.fit(x2)\n",
    "lasso_reg = SelectFromModel(LogisticRegression(penalty = 'l1'))\n",
    "lasso_reg.fit(las_scaler.transform(x2),y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_indexes = []\n",
    "# This code checks how many features were reported to be removed\n",
    "for i in range(len(lasso_reg.get_support())):\n",
    "    if lasso_reg.get_support()[i] == False:\n",
    "        lasso_indexes.append(i)\n",
    "lasso_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This Lasso test was inconclusive as it suggested to keep all 200 features, which obviously is not the best approach when working with Feature Extraction Techniques.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Code here is for Ridge Regression. Ridge regression essentially would be checking the data values that are hurting the model due to multicollinearity, meaning their variances would be distant from the actual true value, so I also used Select Best Model in order to find the features it shows to keep and which ones to remove. I wanted to use Select from model over RFE because in RFE you have to pick how many features to keep, while Select From Model will analyze the all the features and give you a boolean array of what to keep or not. It is different from Lasso, because in Ridge, it shrinks the values TOWARDS zero, versus exactly zero. L2 is for Ridge Regression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to scale it from 0-1 before running it through the model\n",
    "ridge_scaler = StandardScaler()\n",
    "ridge_scaler.fit(x_vals)\n",
    "ridge_reg = SelectFromModel(LogisticRegression(penalty = 'l2'))\n",
    "ridge_reg.fit(ridge_scaler.transform(x_vals),y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the indexes in which the boolean statements were false, so we can remove it\n",
    "ridge_indexes = []\n",
    "for i in range(len(ridge_reg.get_support())):\n",
    "    if ridge_reg.get_support()[i] == False:\n",
    "        ridge_indexes.append(i)\n",
    "len(ridge_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all relvent columns and edit the Data Frame\n",
    "ridgeDrop = []\n",
    "for i in ridge_indexes:\n",
    "    colName = 'var_' + str(i)\n",
    "    ridgeDrop.append(colName)\n",
    "filtered_x = x_vals.drop(columns = ridgeDrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is Extraction for Model 4**. \n",
    "*Model 4 is working on checking which features to use on non-normalized data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgeModel4 = StandardScaler()\n",
    "ridgeModel4.fit(model4Vals)\n",
    "ridge_regModel4 = SelectFromModel(LogisticRegression(penalty = 'l2'))\n",
    "ridge_regModel4.fit(ridgeModel4.transform(model4Vals),y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code below saves the indexes in which to remove from the features\n",
    "# It will return the total number of features needed to remove based on the results\n",
    "ridge_indexes = []\n",
    "for i in range(len(ridge_regModel4.get_support())):\n",
    "    if ridge_regModel4.get_support()[i] == False:\n",
    "        ridge_indexes.append(i)\n",
    "len(ridge_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This iterates through the columns and removes the columns based on the indexes saved and edits the \n",
    "# DataFrame\n",
    "ridgeDrop = []\n",
    "for i in ridge_indexes:\n",
    "    colName = 'var_' + str(i)\n",
    "    ridgeDrop.append(colName)\n",
    "model4Vals = model4Vals.drop(columns = ridgeDrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The bottom code is the code to use Linear Discriminant Analysis in order to see which features can be removed to simplify the data. Linear Discriminant Analysis is primarily used to model differences in groups, thus helping feature extraction products, to reduce the dimensions of the data. By Classifying the features into categories, then usinfg Select Best Model on them, with the boolean array used from get_support, I can find which features to remove from a specific dataset if even necessary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_scaler = StandardScaler()\n",
    "lda_scaler.fit(filtered_x)\n",
    "lda_reg = SelectFromModel(LinearDiscriminantAnalysis())\n",
    "lda_reg.fit(lda_scaler.transform(filtered_x),y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False,  True, False,  True, False,  True,\n",
       "        True,  True, False,  True, False, False, False, False,  True,\n",
       "       False,  True, False,  True, False,  True,  True, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False,  True,  True, False,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_reg.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_indexes = []\n",
    "for i in range(len(lda_reg.get_support())):\n",
    "    if lda_reg.get_support()[i] == False:\n",
    "        lda_indexes.append(i)\n",
    "len(lda_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = list(filtered_x.columns)\n",
    "len(lda_reg.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaDrop = []\n",
    "headers = list(filtered_x.columns)\n",
    "for i,j in zip(lda_reg.get_support(), headers):\n",
    "    if(i == False):\n",
    "        ldaDrop.append(j)\n",
    "LDA_Ridge_data = filtered_x.drop(columns = ldaDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Below for Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_scaler = StandardScaler()\n",
    "m4_scaler.fit(model4Vals)\n",
    "m4_reg = SelectFromModel(LinearDiscriminantAnalysis())\n",
    "m4_reg.fit(m4_scaler.transform(model4Vals),y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True, False,  True,  True, False,\n",
       "        True,  True, False, False,  True, False,  True,  True, False,\n",
       "       False,  True, False,  True, False, False,  True, False,  True,\n",
       "       False, False,  True,  True,  True,  True, False, False, False,\n",
       "       False, False,  True, False,  True, False,  True, False, False,\n",
       "       False,  True,  True,  True,  True, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False,  True, False,\n",
       "       False,  True, False,  True,  True, False, False,  True, False,\n",
       "       False, False, False, False,  True,  True, False,  True,  True,\n",
       "       False, False,  True, False,  True, False,  True, False, False,\n",
       "        True,  True,  True, False, False,  True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_reg.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure the features that the model told me to remove are removed from the dataset\n",
    "model4Vals = model4Vals[model4Vals.columns[m4_reg.get_support()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4's features are a subset of model 5's so I am just copying the data to a new variable to save time\n",
    "# and memory.\n",
    "model5Vals = model4Vals.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Another approach I wanted to takin in terms of feature extraction was to look at the correlation in relation to the actual target value and remove the ones that were least correlated with the target value as they could be helping to overfit the data or they woul not provide any analysis or inference on the data, thus useless.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*However, my findings were that most of the data points were not really closely correlated with the  target value as most of the points had a correlation value of around .05 to -.05. However I just wanted to see if removing the low correlations had a positive affect on some of my models so for one model I removed a decent amount of features when I removed the ones that had a correlation value of less than .025 and greater than -.025. For mey second model that I removed some features using correlation, my threshhold was .045 to -.045, but even then I was only able to remove 3 features based on already the processed data. However, as it can be seen and explained later I did not end up choosing a model that included this correlation extraction model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Also, in the code, you will see that I do cm[0][1:]. This is  because cm is my correlation matrix, and cm[0] is the correlation of my target value's correlation to everything else, and I sliced from the first index down, as the 0'th index is the target in relation to itself which will always be one, but I only care about the features so I removed that one value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.drop(columns = ['ID_code'])\n",
    "corr_df.loc[:,'var_0':'var_199'] = LDA_Ridge_data\n",
    "corr_df = corr_df.dropna(axis = 1, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=np.corrcoef(corr_df.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows no variables have a moderate or strong correlation with the target.\n",
    "remove_i_cols = []\n",
    "lst = cm[0][1:]\n",
    "for i in range(len(lst)):\n",
    "    if lst[i] > -.025 and lst[i] < .025:\n",
    "        remove_i_cols.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "droppedCols = []\n",
    "for colName,data in LDA_Ridge_data.iteritems():\n",
    "    if count in remove_i_cols:\n",
    "        droppedCols.append(colName)\n",
    "    count += 1\n",
    "corr25_ldaRidge = LDA_Ridge_data.drop(columns = droppedCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Extracting Features for kbest_and_corr_features dataset\n",
    "corr_df2 = df.drop(columns = ['ID_code'])\n",
    "corr_df2.loc[:,'var_0':'var_199'] = kbest_and_corr_features\n",
    "corr_df2 = corr_df2.dropna(axis = 1, how = 'all')\n",
    "cm=np.corrcoef(corr_df2.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_i_cols = []\n",
    "lst = cm[0][1:]\n",
    "for i in range(len(lst)):\n",
    "    if lst[i] > -.025 and lst[i] < .025:\n",
    "        remove_i_cols.append(i)\n",
    "        \n",
    "dk = []\n",
    "count = 0\n",
    "for colName,data in kbest_and_corr_features.iteritems():\n",
    "    if count in remove_i_cols:\n",
    "        dk.append(colName)\n",
    "    count += 1\n",
    "kbest_and_corr_features = kbest_and_corr_features.drop(columns = dk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting for Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.drop(columns = ['ID_code'])\n",
    "corr_df.loc[:,'var_0':'var_199'] = model5Vals\n",
    "corr_df = corr_df.dropna(axis = 1, how = 'all')\n",
    "cm=np.corrcoef(corr_df.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_i_cols = []\n",
    "lst = cm[0][1:]\n",
    "for i in range(len(lst)):\n",
    "    if lst[i] > -.045 and lst[i] < .045:\n",
    "        remove_i_cols.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "droppedCols = []\n",
    "for colName,data in model5Vals.iteritems():\n",
    "    if count in remove_i_cols:\n",
    "        droppedCols.append(colName)\n",
    "    count += 1\n",
    "model5Vals = model5Vals.drop(columns = droppedCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the Extraction Code for Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.drop(columns = ['ID_code'])\n",
    "corr_df.loc[:,'var_0':'var_199'] = model6Vals\n",
    "corr_df = corr_df.dropna(axis = 1, how = 'all')\n",
    "cm=np.corrcoef(corr_df.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_i_cols = []\n",
    "lst = cm[0][1:]\n",
    "for i in range(len(lst)):\n",
    "    if lst[i] > -.025 and lst[i] < .025:\n",
    "        remove_i_cols.append(i)\n",
    "        \n",
    "dk = []\n",
    "count = 0\n",
    "for colName,data in model6Vals.iteritems():\n",
    "    if count in remove_i_cols:\n",
    "        dk.append(colName)\n",
    "    count += 1\n",
    "model6Vals = model6Vals.drop(columns = dk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting K Best Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The next extraction approach I applied to some of my model sets of data was using the K Best Features approach. The K best features gets to choos the first k best features that would help predict your test set. Specifically I ran the model on a chi squared distribution, which wouldessentially look at the dependence value in relation to the target values and remove the fesatures that aren't needed. I had to use a min max scalaer for this becasue negative values can't be used for the chi squared function, so I normalized it to a positive number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(kbest_and_corr_features)\n",
    "t = scaler.transform(kbest_and_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3975948 , 0.3140128 , 0.53710406, ..., 0.56464541, 0.31793714,\n",
       "        0.30021665],\n",
       "       [0.48830424, 0.42425939, 0.60193757, ..., 0.6484659 , 0.47743041,\n",
       "        0.51616646],\n",
       "       [0.42435434, 0.45848352, 0.6145575 , ..., 0.527662  , 0.64516098,\n",
       "        0.46396209],\n",
       "       ...,\n",
       "       [0.49828765, 0.3831504 , 0.45381534, ..., 0.55197195, 0.3853606 ,\n",
       "        0.3417794 ],\n",
       "       [0.39400409, 0.27341231, 0.56375533, ..., 0.50476595, 0.50564942,\n",
       "        0.36471573],\n",
       "       [0.50237656, 0.34945227, 0.56641005, ..., 0.70876746, 0.21125289,\n",
       "        0.28487645]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbestcorr = SelectKBest(chi2, k=50)\n",
    "kbestcorr.fit_transform(t, y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = kbestcorr.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "keeps = []\n",
    "for title, b in zip(kbest_and_corr_features.columns,l):\n",
    "    if b:\n",
    "        keeps.append(title)\n",
    "kbest_and_corr_features = kbest_and_corr_features[keeps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the Continuation of the Extraction process for Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(model6Vals)\n",
    "t = scaler.transform(model6Vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42785307, 0.32482435, 0.56805853, ..., 0.56951454, 0.34294267,\n",
       "        0.32765751],\n",
       "       [0.55721218, 0.42863943, 0.6812351 , ..., 0.66807851, 0.53653095,\n",
       "        0.6095461 ],\n",
       "       [0.41196889, 0.48377668, 0.57806091, ..., 0.52249633, 0.64314053,\n",
       "        0.42583343],\n",
       "       ...,\n",
       "       [0.5432771 , 0.39305749, 0.4870996 , ..., 0.55813267, 0.41905483,\n",
       "        0.38087342],\n",
       "       [0.46750324, 0.25309006, 0.66682332, ..., 0.51568196, 0.58482497,\n",
       "        0.46661746],\n",
       "       [0.5258457 , 0.36714503, 0.580254  , ..., 0.70933049, 0.22572933,\n",
       "        0.29469964]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk6 = SelectKBest(chi2, k=50)\n",
    "mk6.fit_transform(t, y_vals) # Scaled it so that Non-Negative Values will not Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = mk6.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on what the model told me to keep, I am keeping the relavent features.\n",
    "keeps = []\n",
    "for title, b in zip(model6Vals.columns,l):\n",
    "    if b:\n",
    "        keeps.append(title)\n",
    "model6Vals = model6Vals[keeps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code below takes in a 2x2 matrix (a confusion matrix) and will return the true negatives and true positives to help simplify that analyzing porocess of a Confusion Matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_Confusion(lst):\n",
    "    negatives = sum(lst[0])\n",
    "    positives = sum(lst[1])\n",
    "    tn = lst[0][0]/negatives\n",
    "    tp = lst[1][1]/positives\n",
    "    print(f'Model predicted Zeros {tn*100}% correctly.')\n",
    "    print(f'Model predicted Ones {tp*100}% correctly.')\n",
    "    return (tn*100, tp*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following arrays are to keep track of the scores and accuracy of each parameter to make is visually appealable and comparable afterwards.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "ones = []\n",
    "zeros = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Below are the ten models I trained with various test sets with different combinations of features and also different classifiers such as SVM and Logistic Regression. Although in its name \"regression,\" it is a classifier. I have labelled which feature extraction techniquest I used above each model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only Ridge and LDA for Logistic AFTER NORMALIZING (Model 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a Logistic Regression Model*\n",
    "\n",
    "*For this dataset, I first used ridge regression and called select best model on it, which suggested to remove about over 100 features, so I did so. Then the next feature extraction technique that I used was LDA, which I described above. This brough the used features to this model to 20 Features.*\n",
    "\n",
    "*The model as it can be seen through the Confusion matrix and score was predicting almost 90% correctly. So, it shows that it is not overfitting. Although it is predicting almost all 0's correctly, it is predicting almost all 1's incorrectly, which explains why the accuracy is 90% because about 90% of the values are 0's*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "final_model = LogisticRegression(penalty = 'l2').fit(LDA_Ridge_data,y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fval = final_model.predict(LDA_Ridge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899675"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = final_model.score(LDA_Ridge_data,y_vals)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[179886     16]\n",
      " [ 20049     49]]\n",
      "Model predicted Zeros 99.99110626896866% correctly.\n",
      "Model predicted Ones 0.24380535376654394% correctly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y_vals, fval)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1Columns = LDA_Ridge_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only Ridge, LDA, and Correlation w/ threshold of +-.025 for Logistic AFTER NORMALIZING (Model 2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a Logistic Regression Model*\n",
    "\n",
    "*For this data I used ridge regression, and LDA to remove features up to 20, then I also removed the correlations that were very close to zero even though many were less than .05, so I used .025 to -.025. This one also had an accuracy of 90% about, and as it can be seen through correlation matrix, it was barely predictiong the ones correctly, and most predictions was 0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rlc = LogisticRegression(penalty = 'l2').fit(corr25_ldaRidge,y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlc_prediction = rlc.predict(corr25_ldaRidge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899675"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = rlc.score(corr25_ldaRidge,y_vals)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[179886     16]\n",
      " [ 20049     49]]\n",
      "Model predicted Zeros 99.99110626896866% correctly.\n",
      "Model predicted Ones 0.24380535376654394% correctly.\n"
     ]
    }
   ],
   "source": [
    "# Don't use this model because it is only predicting 2 ones\n",
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y_vals, rlc_prediction)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2Columns = corr25_ldaRidge.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Correlation and K Best Feature Selector AFTER NORMALIZING (Model 3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a Logistic Regression Model*\n",
    "\n",
    "*For selecting the features for this model, I used the correlation techniques and k best selector as I described the process in the EDA section. For correlation, I removed values that were less than .025 and greater than -.025 as they are super insignificant. Then I selected the 50 best features from k-best. This showed to be a little more accurate due to score, as it was able to predict more ones correctly as well. As it can be seen through the confusion matrix, this model predicted a few more ones properly, which is up to 7%.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_corrkbest = LogisticRegression(penalty = 'l2').fit(kbest_and_corr_features,y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_corrkbest_prediction = clf_corrkbest.predict(kbest_and_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90413"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = clf_corrkbest.score(kbest_and_corr_features,y_vals)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[179328    574]\n",
      " [ 18600   1498]]\n",
      "Model predicted Zeros 99.6809373992507% correctly.\n",
      "Model predicted Ones 7.453477958005772% correctly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y_vals, clf_corrkbest_prediction)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3Columns = kbest_and_corr_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only Ridge and LDA for Logistic WITHOUT NORMALIZING (Model 4)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a Logistic Regression Model*\n",
    "\n",
    "*For the next 3 models, I did not normalize the data. This was because I was realizing that as I was normalizing the data, the ranges only max out at 1, and because we have a binary target with already less amount of ones, it wasn't able to recognize the 1 values as they may have been outliers, but by not normalizing, the features were able to provide more information in deciding the one values as well.*\n",
    "\n",
    "*I used the same approach for picking these features as I did for the first model, but the difference was that I did not normalize the data. So, different features were used. It can be seen that now, it was predicting a few more ones correctly rather than predicting all zero's right or wrong.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model4 = LogisticRegression(penalty = 'l2').fit(model4Vals,y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_predictions = model4.predict(model4Vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90508"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = model4.score(model4Vals,y_vals)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178782   1120]\n",
      " [ 17864   2234]]\n",
      "Model predicted Zeros 99.37743882780626% correctly.\n",
      "Model predicted Ones 11.115533883968553% correctly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y_vals, model4_predictions)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4Columns = model4Vals.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only Ridge, LDA, and Correlation w/ threshold of +-.045 for Logistic WITHOUT NORMALIZING (Model 5)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a Logistic Regression Model*\n",
    "\n",
    "*As explained previously, I did not want to use normalized data to see the difference, so I used the same approach as I did for the second model, except now using non-normalized data, and also a higher correlation threshhold to remove more features. This score was 90%, but was predicting a less amount of ones overall in comparision to the previous model. However, still performs better than the models that did use normalization techniques.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model5 = LogisticRegression(penalty = 'l2').fit(model5Vals,y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5_predictions = model5.predict(model5Vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90447"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = model5.score(model5Vals,y_vals)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178843   1059]\n",
      " [ 18047   2051]]\n",
      "Model predicted Zeros 99.41134617736324% correctly.\n",
      "Model predicted Ones 10.204995521942482% correctly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y_vals, model5_predictions)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5Columns = model5Vals.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Correlation and K Best Feature Selector WITHOUT NORMALIZING (Model 6)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a Logistic Regression Model*\n",
    "\n",
    "*For the same reasons explained above, I wanted to try theis model without normaliing the data. I used the same approach of feature extraction as the third model, but the only difference was that I was using non-normalized data. This was the model that predicted the number of 1's the most for Logistic regressions, but I did not want to use the model if it extracted using correlation unless it was **significantly** better than another model. The reason for this is because all of the features have significantly low correlations, removing some that are close to 0 isn't that good of a strategy as all the points are far less than .1, so a correlation of .03 wouldn't help much more than a correlation of .02, which is why I wanted to use another model that had more of a mathematical reason to extract features. This also wasnt much more effective in predicting the 1's correctly, so I felt like the model just got lucky.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model6 = LogisticRegression(penalty = 'l2').fit(model6Vals,y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_predictions = model6.predict(model6Vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.906095"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = model6.score(model6Vals,y_vals)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178585   1317]\n",
      " [ 17464   2634]]\n",
      "Model predicted Zeros 99.26793476448289% correctly.\n",
      "Model predicted Ones 13.105781669817892% correctly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y_vals, model6_predictions)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "m6Columns = model6Vals.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM Classifier Using only Ridge and LDA for Logistic AFTER NORMALIZING (Model 7)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The next 4 models are Support Vector Machine Models (SVM). SVM is a linear classifier that essentially classifies the points based on the the plane the point lies on in order to classify it as a specific class. I had to run a sample of the train data for 20% of the data as the computational time was extremely long and too much for my computer to process at a stretch.*\n",
    "\n",
    "*I am using the same features extracted using the model 1 feature extraction process, with normalized sata. Essentially, the data being inputted into this model is the same as that of model one's. The score is 90%, but as I was looking deeper, it is 90% because it did not predict a single one, and predicted all zero's for the set of data, which is why the score is decieving , but there are 90% 0's in the data, which is why the score is decieving. This was one of the worst models, as well as took too much computational time, which is why I had to run the model on a random sample of 20% of the data, but the results were the same with 20% of the data or 100% of the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmCLF = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1Cols = LDA_Ridge_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigData = df.sample(frac = .2).reset_index(drop = True)\n",
    "y = bigData['target']\n",
    "x = normalize(bigData[svm1Cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmCLF.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClf_predicted = svmCLF.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9012"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = svmCLF.score(x,y)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36048     0]\n",
      " [ 3952     0]]\n",
      "Model predicted Zeros 100.0% correctly.\n",
      "Model predicted Ones 0.0% correctly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y, svmClf_predicted)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only Ridge, LDA, and Correlation w/ threshold of +-.025 for Logistic AFTER NORMALIZING (Model 8)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I used the SVM Classifier for this model.*\n",
    "\n",
    "*I used the same data that was fed into the second Logistic Regression model into this Classifier, however the results showed to be not effective at all. The mdodel was prediting all 0's and not a single 1, and the reason the score looks good is because the majority of the target values were 0's. This is why we have to look at more of the metrics such as the confusion matrix and analyze it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmCLF = SVC(gamma='auto') #BY NOT NORMALIZING, THE SET WAS AMAZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigData = df.sample(frac = .2).reset_index(drop = True)\n",
    "y = bigData['target']\n",
    "x = normalize(bigData[m2Columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmCLF.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClf_predicted = svmCLF.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc = svmCLF.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35976     0]\n",
      " [ 4024     0]]\n",
      "Model predicted Zeros 100.0% correctly.\n",
      "Model predicted Ones 0.0% correctly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y, svmClf_predicted)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM Using Correlation and K Best Feature Selector AFTER NORMALIZING (Model 9)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I used the SVM Classifier for this model.*\n",
    "\n",
    "*I used the same dataset as I did for the third Logistic Regression model, as I wanted to see how the SVM model performed on the data with similar features. It did not perform well as seen with the previous SVM models, that the model is predicting all 0's regardless of the x-values, and is not even predicting a one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigData = df.sample(frac = .3).reset_index(drop = True)\n",
    "y = bigData['target']\n",
    "x = normalize(bigData[m3Columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9_predicted = model9.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8998333333333334"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = model9.score(x,y)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53990     0]\n",
      " [ 6010     0]]\n",
      "Model predicted Zeros 100.0% correctly.\n",
      "Model predicted Ones 0.0% correctly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y, model9_predicted)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only Ridge, LDA, and Correlation w/ threshold of +-.025 for Logistic WITHOUT NORMALIZING (Model 10)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I used the SVM Classifier for this model.*\n",
    "\n",
    "*I used the same non-normalized data of the features being extracted from the technique of model 4, and ran this model with those features. I wanted to try running the SVM on non-normalized values for the same reason as I tried it with the regression models in the fact that by normalizing the data, the model would not be able to recognize the outlier values as all of the x-values would be too close to one another to distinguish the outliers.*\n",
    "\n",
    "*This model at first seems to be the best performing as it has a score of 100%, and predicted all of the 1's and zero's correctly of the train set. However, it is learly visible that the model is overfitting the data as it should never be 100% on a training set, or else it means your model is too unique and is only for this data in specific, but will be inaccurate in predicting other data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigData = df.sample(frac = .2).reset_index(drop = True)\n",
    "y = bigData['target']\n",
    "x = bigData[m4Columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model10.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10_predicted = model10.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = model10.score(x,y)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36048     0]\n",
      " [    0  3952]]\n",
      "Model predicted Zeros 100.0% correctly.\n",
      "Model predicted Ones 100.0% correctly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x = confusion_matrix(y, model10_predicted)\n",
    "print(x)\n",
    "(z, o) = analyze_Confusion(x)\n",
    "scores.append(sc)\n",
    "ones.append(o)\n",
    "zeros.append(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Scores of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Table below shows the score of each model, and how accurate they predicted correct 1's, and correct 0's.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Scores</th>\n",
       "      <th>Percentage of Correct 1s (%)</th>\n",
       "      <th>Percentage of Correct 0s (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model 1</th>\n",
       "      <td>0.899675</td>\n",
       "      <td>0.243805</td>\n",
       "      <td>99.991106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 2</th>\n",
       "      <td>0.899675</td>\n",
       "      <td>0.243805</td>\n",
       "      <td>99.991106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 3</th>\n",
       "      <td>0.904130</td>\n",
       "      <td>7.453478</td>\n",
       "      <td>99.680937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 4</th>\n",
       "      <td>0.905080</td>\n",
       "      <td>11.115534</td>\n",
       "      <td>99.377439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 5</th>\n",
       "      <td>0.904470</td>\n",
       "      <td>10.204996</td>\n",
       "      <td>99.411346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 6</th>\n",
       "      <td>0.906095</td>\n",
       "      <td>13.105782</td>\n",
       "      <td>99.267935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 7</th>\n",
       "      <td>0.901200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 8</th>\n",
       "      <td>0.899400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 9</th>\n",
       "      <td>0.899833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model Scores  Percentage of Correct 1s (%)  \\\n",
       "Model 1       0.899675                      0.243805   \n",
       "Model 2       0.899675                      0.243805   \n",
       "Model 3       0.904130                      7.453478   \n",
       "Model 4       0.905080                     11.115534   \n",
       "Model 5       0.904470                     10.204996   \n",
       "Model 6       0.906095                     13.105782   \n",
       "Model 7       0.901200                      0.000000   \n",
       "Model 8       0.899400                      0.000000   \n",
       "Model 9       0.899833                      0.000000   \n",
       "Model 10      1.000000                    100.000000   \n",
       "\n",
       "          Percentage of Correct 0s (%)  \n",
       "Model 1                      99.991106  \n",
       "Model 2                      99.991106  \n",
       "Model 3                      99.680937  \n",
       "Model 4                      99.377439  \n",
       "Model 5                      99.411346  \n",
       "Model 6                      99.267935  \n",
       "Model 7                     100.000000  \n",
       "Model 8                     100.000000  \n",
       "Model 9                     100.000000  \n",
       "Model 10                    100.000000  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Model Scores' : scores, \n",
    "                  'Percentage of Correct 1s (%)' : ones,\n",
    "                  'Percentage of Correct 0s (%)' : zeros})\n",
    "df.index = ['Model 1','Model 2','Model 3','Model 4','Model 5','Model 6','Model 7','Model 8','Model 9','Model 10']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Test Set & ML Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I did not want to use model 10 as this model clearly seemed to overfit, and I did not want to use any of the SVM models (7,8,9) as they never were predicting ones at all. In addition, the computational cost of using the SVM models is extremely more than the cost of using Logistic Regression, so SVM would be a bad approach to use to predict the test values in my opinion. This left me with 6 models to choose from (1-6), however, I elimenated the first three models to choose from as they relied on normalized data to be pased through to the model, but as I stated before, I felt that the normalized data did not help the model in predicting ones effectively in comparasion with the other models. Then, after looking at the scores and the confusion matrices, I was down to picking between model 4 and 6. Model 4 used Ridge regression and an LDA in order to extract features and simplify the data, while model 6 used K Best Features and removed low correlations to extract features. I however, felt uncomfortable using a model that removed features through correlation because all of the features had a correlation of plus or minus of .1 from 0, meaning that the correlation was little to none in relation to the target, so by removing a point with a correlation of .02 in comparasion to .03 would make little to no difference in my opinion. This is what led me to choose model 4 over model 6 even though model 6 predicts the percentage of ones 2% more correctly than model 4, but I felt that the features were picked with more reasoning in model 4, and could potentially outperform 6 as model 6 could have just gotten lucky. This is generally the bias-variance tradeoff that I was trying to explain, as I feel like model 4 has a more genaralized view than model 6, and can predict better on actual sets it hasn't seen before. Even though model 10 predicted 100%, it is too simple and will not genaralize on data it hasn't seen before which is why it is not a good model as well. Initially another challenge I had was that when I ran all of my models, I thought they were all great and I didn't know what to choose from, but by looking at their accuracy and confusion matrix, I got to see thie true positives and true negatives properly. Another challenge was because the SVM's take too much time to run when big data wis given to them, I had to find a way to randomlly sample the data and train them with a sample of the large data. The computational time was part of the reason the SVM models seemed unappealing. *\n",
    "\n",
    "*Below is my model 4, which I believe is my best model, predicting on the test set.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only Ridge and LDA for Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv(\"test.csv\")\n",
    "y_test = pd.read_csv(\"test_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.drop(columns = 'ID_code')\n",
    "test_drop = x_test[m4Columns]\n",
    "#test_drop.append('var_17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model4.predict(test_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986885"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[197377   2623]\n",
      " [     0      0]]\n"
     ]
    }
   ],
   "source": [
    "x = confusion_matrix(y_test, predicted)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
